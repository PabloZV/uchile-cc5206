---
title: "Tutorial 3: Clustering"
author: "Mauricio Quezada, José Miguel Herrera, Bárbara Poblete"
date: "14 de Octubre 2017"
output: 
  html_document: 
    theme: spacelab
    toc: yes
---

# Clustering

En estos 2 laboratorios usaremos métodos de clustering para particionar datos.  

## Kmeans

El método k-means es un simple y **elegante** método para particionar datos en distintos clusters. Para ejemplificar, y conocer las funciones de R, haremos un ejemplo práctico donde se ven claramente 2 clusters:

```{r}
set.seed(2)
x = matrix(rnorm(50*2), ncol = 2)  # crea matriz de 2 columnas con 50 valores c/u aleatoriamente
x[1:25,1] = x[1:25,1] + 3   # los primeros 25 se les suma 3
x[1:25,2] = x[1:25,2] - 4   # los siguientes 25 se les resta 4
head(x)
```

Ejecutamos k-means y le indicamos que queremos que divida los datos en 2 clusters:
```{r}
km.out <- kmeans(x, 2, nstart = 20)
```

El parámetro *nstart* indica cuantas veces queremos que se ejecute el metódo desde cero. Se reporta siempre el mejor resultado.

Las asignaciones de cada una de las 50 observaciones están contenidas en *k$cluster*:
```{r}
km.out$cluster
```

También podemos saber el tamaño de cada cluster:
```{r}
km.out$size
```

O donde están ubicados los *centroides*:
```{r}
km.out$centers
```


### Estimando la cantidad de clusters

En el ejemplo, creamos los datos y los separamos en dos partes de manera intencional. Sin embargo, kmeans necesita el parámetro de la cantidad de clusters y no siempre sabremos la cantidad de particiones que se pueden hacer. 

Una forma es estimandolo mediante la suma de la diferencia al cuadrado entre los puntos de cada cluster (wss). 

```{r}
wss <- 0
clust = 15 # graficaremos hasta 15 clusters
for (i in 1:clust){
  wss[i] <-
    sum(kmeans(x, centers=i)$withinss)
}

plot(1:clust, wss, type="b", xlab="Numero de clusters", ylab="wss")
```

Acá se puede notar que un valor óptimo es 2 (mirar donde se forma un "codo"). Si eligieramos 3,4 ó 5, veríamos más particiones, pero no serían tan claras. ¡Podrían probarlo!



#### Graficando

Podemos ver si hay grupos entre pares de variables usando `pairs`. Acá notamos que hay una separación de 2 clusters:
```{r}
pairs(x)
```

También podríamos ver cada cluster con colores:
```{r}
pairs(x, col=km.out$cluster)
```


También lo podríamos haber hecho con *plot* (puesto que son sólo 2 variables):
```{r}
plot(x, main="Resultados usando k = 2", xlab="", ylab="")
```

Lo mismo en colores: 
```{r}
plot(x, col=(km.out$cluster), main="Resultados usando k = 2 (plot bonito)", xlab="", ylab="", pch=20, cex=2)
```





### Otra forma de graficar los resultados

Emplearemos un pequeño dataset del ingreso per cápita (IPCAP) en dólares de ciertos países en el año 2015 y 2016. Trataremos de particionar los datos de tal manera que deje los países más con mayor ingreso per cápita en un cluster y el resto en otro cluster.  

```{r}
d <- read.csv("/Users/jota/git/tutoriales/ip.txt", row.name = 1, header = T)
head(d)

#SSE
wss <- 0
clust = 15
for (i in 1:clust){
  wss[i] <-
    sum(kmeans(d, centers=i)$withinss)
}

plot(1:clust, wss, type="b", xlab="Número de clusters", ylab="wss")
```

En este caso, se puede apreciar el codo el k = 2. Por lo tanto, creamos los clusters con k = 2. 
```{r}
k2 <- kmeans(d, 2, nstart=20)
```

Una forma de graficar: 

```{r}
library(ggplot2)
ggplot(d, aes(anio2015, anio2016, color = k2$cluster)) +  geom_point() 
```

Otra forma de graficar (más clara): 

```{r}
library("cluster")
clusplot(d, k2$cluster, color=TRUE, shade=TRUE, labels=2, lines=0, main="IPCAP (k=2)")
```


Hagamos lo mismo pero ahora con k = 3:

```{r}
k2 <- kmeans(d, 3, nstart=20)
clusplot(d, k2$cluster, color=TRUE, shade=TRUE, labels=2, lines=0, main="IPCAP (k=3)")
```


Hagamos lo mismo pero ahora con k = 4:


```{r}
k2 <- kmeans(d, 4, nstart=20)
clusplot(d, k2$cluster, color=TRUE, shade=TRUE, labels=2, lines=0, main="IPCAP (k=4)")
```


## Clustering Jerárquico (Hierarchical clustering)

Otra forma de hacer clustering es mediante clustering Jerárquico. En R la función `hclust()` permite utilizar este método. Usaremos el ejemplo del principio que está almacenado en la variable `x` para graficar un dendrograma usando las 3 técnicas del método: complete, single y average. Usaremos además la distancia euclideana para medir distancias https://en.wikipedia.org/wiki/Euclidean_distance. 

```{r}
hc.complete <- hclust(dist(x), method = "complete")
hc.single <- hclust(dist(x), method = "single")
hc.average <- hclust(dist(x), method = "average")
```

Ahora veamos los dendrogramas:
```{r}
par(mfrow=c(1,3))
plot(hc.complete, main="Complete", xlab="", ylab="", cex=.9)
plot(hc.single, main="Single", xlab="", ylab="", cex=.9)
plot(hc.average, main="Average", xlab="", ylab="", cex=.9)
```


Para ver la asignación de cada observación a un cluster:

```{r}
cutree(hc.complete, 2) # si el arbol se corta en el segundo nivel
cutree(hc.single, 2)   # ssi el arbol se corta en el segundo nivel
cutree(hc.average, 2)  # si el arbol se corta en el segundo nivel
```


Para escalar las variables antes de hacer clustering jerárquico de las observaciones:

```{r}
xec = scale(x)
plot(hclust(dist(xec), method = "complete"), main = "HC complete (scaled features)")
```


## DBSCAN

```{r, eval=F}
#install.packages("dbscan")
```


Usaremos el dataset de iris:

```{r}
library("dbscan")
data("iris")
x <- as.matrix(iris[, 1:4])
db <- dbscan(x, eps = .4)  # eps es el radio
db
```

Graficamos
```{r}
pairs(x, col = db$cluster + 1L)
```


```{r}
lof <- lof(x, k = 4)
pairs(x, cex = lof)
```

## Otras bibliotecas útiles para la sesión

```{r, eval=F}
#install.packages("data.table") # si es necesario
```


## Referencias complementarias

* Document clustering with python:  http://brandonrose.org/clustering

